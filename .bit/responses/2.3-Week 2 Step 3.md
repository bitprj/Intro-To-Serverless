---
files: n/a
stepType: checks
scripts: test.2.3.js
week: 2
step: 3
name: Week 2 Step 3
---
Week 2 Step 3 ‚¨§‚¨§‚¨§‚óØ‚óØ‚óØ‚óØ‚óØ | üïê Estimated completion: 10-30 minutes

# Getting Emotional ~ With the Face API
> Remember back in step 1 we used Postman to call the API? Well we are going to call the Face API with our Azure Function!

## ‚úÖ Task:
**Create an Azure function that can send a static image to the Face API that returns emotion data**
- [ ] ***1:*** Add Face API Keys and endpoint into your Function App Secrets
- [ ] ***2:*** Edit your function to make a request to the Face API using `node-fetch` and send back the emotion data in the body in **JSON format** ```body: {the_data}```
- [ ] üöÄ Place your function URL in a [repository secret](https://docs.github.com/en/actions/reference/encrypted-secrets#creating-encrypted-secrets-for-a-repository) called `EMOTIONAL_ENDPOINT` and commit your code to `emotionalgifs/index.js` on the `emotionalgifs` branch

## üöß Test your Work
Use **Postman**! Paste the *function url* and make a POST request. Remember to attach the file in `Body`! In the output box, you should get the output. Make sure you're using an image with a *real face* on it or else it won't work. Here's an example of an output I get with [this image](https://user-images.githubusercontent.com/69332964/98884689-91687580-245e-11eb-98d7-6461ac79e02a.jpg):
<details>
<summary>:white_check_mark: Expected Output </summary>
The output should be in JSON format like so:
  
```json
{
  "result": [
    {
      "faceId": "a16f522d-0577-4e50-97cb-1feeef7aaf2e",
      "faceRectangle": {
        "top": 313,
        "left": 210,
        "width": 594,
        "height": 594
      },
      "faceAttributes": {
        "emotion": {
          "anger": 0,
          "contempt": 0,
          "disgust": 0,
          "fear": 0,
          "happiness": 1,
          "neutral": 0,
          "sadness": 0,
          "surprise": 0
        }
      }
    }
  ]
}
```
</details>

---

## 1. Define keys to authenticate the request

This function takes in one parameter, `img`, that contains the image we're trying to analyze.  Inside, we have two variables involved in the call: `subscriptionKey`  and `uriBase`.  

The `process.env` object allows you to access super-secret values in your backend. This prevents hackers from getting your keys and doing bad stuff (like exploiting your resources).

```js
async function analyzeImage(img){
    const subscriptionKey = process.env.SUBSCRIPTIONKEY;
    const uriBase = process.env.ENDPOINT + '/face/v1.0/detect';
}
```

Follow this [Documentation](https://docs.microsoft.com/en-us/azure/azure-functions/functions-how-to-use-azure-function-app-settings?tabs=portal) to save your keys in your Function App.
- [Get started in the Azure portal](https://docs.microsoft.com/en-us/azure/azure-functions/functions-how-to-use-azure-function-app-settings?tabs=portal#get-started-in-the-azure-portal)
- [Work with application settings](https://docs.microsoft.com/en-us/azure/azure-functions/functions-how-to-use-azure-function-app-settings?tabs=portal#settings)


## 2: Call the FACE API

Create a new function `analyzeImage(img)` outside of  `module.exports`  that will handle analyzing the image. Keep in mind this function is **async** because we will be using the **await** keyword with the API call.

> Get your Face API keys ready! We need to let our Face API know that we are authenticated to access this resource.

<details>
<summary>‚ùì What is an async function?</summary>
</br>

Javascript is what we call a "synchronous" language, meaning operations in Javascript block other operations from executing until they are complete, creating a sense of single directional flow. **This means that only one operation can happen at a time.** However, in order to maximize efficiency (save time and resources), Javascript allows the use of asynchronous functions.

Async functions allow other operations to continue running as they are being executed. Refer to [this blog](https://dev.to/hardy613/asynchronous-vs-synchronous-programming-23ed) for more information.

Promises are sychnronous objects, similar to their real life meaning, return a value at some point in the future, or a reason for why that value could not be returned - they represent the result of an async function that may or may not be resolved.

> [Is JavaScript Synchronous or Asynchronous? What the Heck is a Promise?](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise)

> [Master the JavaScript Interview: What is a Promise?](https://medium.com/better-programming/is-javascript-synchronous-or-asynchronous-what-the-hell-is-a-promise-7aa9dd8f3bfb)

</br>
</details>

<details>
<summary>‚ùóÔ∏è Specify the parameters for your request!</summary>
</br>

In order to specify all of our parameters easily, we're going to create a new `URLSearchParams`  object. Here's the object declared for you. I've also already specified one parameter, `returnFaceId`,  as `true` to provide an example. Add in a new parameter that requests emotion.

Remember, these parameters are coming from the [Face API documentation](https://westus.dev.cognitive.microsoft.com/docs/services/563879b61984550e40cbbe8d/operations/563879b61984550f30395236)!

<img width="750" alt="Screen Shot 2021-05-16 at 9 20 19 PM" src="https://user-images.githubusercontent.com/70852990/118420351-a384d400-b68c-11eb-850b-04514c883345.png">

```js
let params = new URLSearchParams({
	'returnFaceId': 'true',
	'<PARAMETER NAME>': '<PARAMETER VALUE>'     //FILL IN THIS LINE
})
```

</details>

#### :hammer_and_wrench: Calling the API

In the same way we installed `parse-multipart`, install `node-fetch`.
Read the [**API** section](https://www.npmjs.com/package/node-fetch#api) of the documentation. We're going to make a **call** using the `fetch(url, {options})` function.

To call the `fetch` function - use the **await** keyword, which we need because `fetch` returns a **Promise**, which is a proxy for a value that isn't currently known. You can read about Javascript promises [here](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise). 

> API Documentation can be tricky sometimes...Here's something to [help](https://learn.parabola.io/docs/reading-api-docs)

:bulb: **Request Headers** tell the receiving end of the request what *type* of data is in the body.

<details>
<summary>‚ùì How do I specify Request Headers?</summary>
</br>

Go back to the Face API documentation [here](https://westus.dev.cognitive.microsoft.com/docs/services/563879b61984550e40cbbe8d/operations/563879b61984550f30395236), and find the **Request headers** section. 

There are two headers that you need. I've provided the format below. Enter in the two header names and their two corresponding values. 

FYI: The `Content-Type`  header should be set to`'application/octet-stream'`.  This specifies a binary file.

```js
    //COMPLETE THE CODE
    let resp = await fetch(uriBase + '?' + params.toString(), {
        method: '<METHOD>',  //WHAT TYPE OF REQUEST?
        body: '<BODY>',  //WHAT ARE WE SENDING TO THE API?
      
      	//ADD YOUR TWO HEADERS HERE
        headers: {
            '<HEADER NAME>': '<HEADER VALUE>'
        }
    })
```

</details>

<details>
<summary>‚ùì What is the URL?</summary>
</br>

Notice that the URL is just the uriBase with the params we specified earlier appended on.

`const uriBase = '<YOUR ENDPOINT>' + '/face/v1.0/detect';`

</details>

<details>
<summary>‚ùì Still Confused? </summary>

Fill in the `method`  and `body`.
	
```js
async function analyzeImage(img){
    
    const subscriptionKey = '<YOUR SUBSCRIPTION KEY>';
    const uriBase = '<YOUR ENDPOINT>' + '/face/v1.0/detect';
    let params = new URLSearchParams({
        'returnFaceId': 'true',
        'returnFaceAttributes': 'emotion'
    })
    //COMPLETE THE CODE
    let resp = await fetch(uriBase + '?' + params.toString(), {
        method: '<METHOD>',  //WHAT TYPE OF REQUEST?
        body: '<BODY>',  //WHAT ARE WE SENDING TO THE API?
        headers: {
            '<HEADER NAME>': '<HEADER VALUE>'  //do this in the next section
        }
    })
    let data = await resp.json();
    
    return data; 
}
```
</details>
	
#### ‚è¨ Receiving Data

Call the `analyzeImage` function in `module.exports`. Add the code below into `module.exports`.

Remember that `parts` represents the parsed multipart form data. It is an `array` of parts, each one described by a filename, a type and a data. Since we only sent one file, it is stored in index 0, and we want the `data`  property to access the binary file‚Äì hence `parts[0].data`. Then in the HTTP response of our Azure function, we store the result of the API call.

```js
//module.exports function
//analyze the image
var result = await analyzeImage(parts[0].data);
context.res = {
	body: {
		result
	}
};
console.log(result)
context.done(); 
```
