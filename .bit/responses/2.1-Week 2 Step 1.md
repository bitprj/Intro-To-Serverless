---
files: n/a
stepType: checks
scripts: test.parseimage.js
week: 2
step: 1
name: Week 2 Step 1
---

**Week 1 Step 1** ‚¨§‚óØ‚óØ

üïê **Estimated completion:** 1-2 hours

## Emotion Analysis

### ‚úÖ Task:
**Create an azure function that can send a static image to the face API that returns if a person has a beard or not.**
- [ ] Create a new branch named `week-2` (use this branch for all tasks in week 2)
- [ ] Create a new Azure function
- [ ] Create a Face [API Endpoint](https://smartbear.com/learn/performance-monitoring/api-endpoints/#:~:text=For%20APIs%2C%20an%20endpoint%20can,to%20carry%20out%20their%20function.&text=The%20place%20that%20APIs%20send,lives%2C%20is%20called%20an%20endpoint.)
- [ ] Set up the Emotion Reader Azure Function
- [ ] Build an Azure function that makes a request to the Face API and analyzes an image
- [ ] Take a screenshot of your Postman response for the beard 

## ‚ùì If you are confused

<details>
<summary>‚ùì What does the Face API do?</summary>
</br>

The Face API will accept the image and return information about the face, specifically emotions. Watch this video on Microsoft Cognitive Services for an in-depth explanation: http://www.youtube.com/watch?v=2aA8OEZ1wk8 
</details>

<br>

<details>
<summary>‚ùì How do I create a new Azure Function? </summary>

- [ ] Create **a new HTTP trigger function** in your Azure portal along
- [ ] Add your **new Function url** to a GitHub secret named `FUNCTION_URL`
</details>
<br>

<details>
<summary>‚ùì How do I create and access the Face API?</summary>
</br>

1. Log into your Azure portal
2. Navigate to **Create a Resource**, the **AI + Machine Learning** tab on the left, and finally select **Face** and fill out the necessary information
3. Record and save the API endpoint and [subscription key](https://docs.microsoft.com/en-us/azure/api-management/api-management-subscriptions)
4. Place the API endpoint and subscrition key in the GitHub repository secrets: `API_ENDPOINT` AND `SUBSCRIPTION_KEY`
    * These keys will be used in the Azure function to give access to this API

<br>

<details>
<summary>‚ùì Where can I find the Face API keys?</summary>
 </br>

1. Navigate to the home page on the Micrsoft Azure portal (https://portal.azure.com/#home)
<img width="1440" alt="Screen Shot 2021-02-04 at 4 00 33 PM" src="https://user-images.githubusercontent.com/28051494/106971033-edac0000-6702-11eb-8243-1b5c2318f76d.png">

2. Click on the resource you need the keys for
<img width="1438" alt="Screen Shot 2021-02-04 at 4 00 49 PM" src="https://user-images.githubusercontent.com/28051494/106971035-ef75c380-6702-11eb-965b-c3ef7b5a7574.png">

3. On the left menu bar, locate the Resource Management section and click on "Keys and Endpoint"
<img width="1440" alt="Screen Shot 2021-02-04 at 12 26 36 PM" src="https://user-images.githubusercontent.com/28051494/106971042-f43a7780-6702-11eb-9e28-e6b2bc16fa22.png">
<br>
</details>

</details>
<br>

<details>
<summary>‚ùì How do I make the request to the Face API?</summary>
</br>

Create a new function in you new Azure function index.js file, outside of  `module.exports`  that will handle analyzing the image (this function is **async** because we will be using the **await** keyword with the API call).

This function will be called `analyzeImage(img)` and takes in one parameter, `img`, that contains the image we're trying to analyze.  Inside, we have two variables involved in the call: `subscriptionKey`  and `uriBase`.  Substitute the necessary values with your own info.

```js
async function analyzeImage(img){
    const subscriptionKey = process.env.SUBSCRIPTIONKEY;
    const uriBase = process.env.ENDPOINT + '/face/v1.0/detect';
}
```
</details>
<br>

<details>
<summary>‚ùì What are the parameters for the request?</summary>
</br>

Checkout the [documentation for the Face API](https://westus.dev.cognitive.microsoft.com/docs/services/563879b61984550e40cbbe8d/operations/563879b61984550f30395236). Notice that the request url is this:

`https://{endpoint}/face/v1.0/detect\[?returnFaceId]\[&returnFaceLandmarks]\[&returnFaceAttributes]\[&recognitionModel]\[&returnRecognitionModel][&detectionModel]`

with the following parameters in [ ]:
* [?returnFaceId]
* [&returnFaceLandmarks]
* [&returnFaceAttributes]
* [&recognitionModel]
* [&returnRecognitionModel]
* [&detectionModel]

All of the bracketed sections represent possible request parameters. Read through **Request Parameters** section carefully. How can we specify that we want to get the emotion data?

</details>
<br>

<details>
<summary>‚ùóÔ∏è Specify parameters in your new function?</summary>
</br>

In order to specify all of our parameters easily, we're going to create a new `URLSearchParams`  object. Here's the object declared for you. I've also already specified one parameter, `returnFaceId`,  as `true` to provide an example. Add in a new parameter that requests emotion.

```js
let params = new URLSearchParams({
	'returnFaceId': 'true',
	'<PARAMETER NAME>': '<PARAMETER VALUE>'     //FILL IN THIS LINE
})
```

</details>
<br>

<details>
<summary>‚ùóÔ∏è Use Fetch in order to send a post request to the Face API Endpoint and receive emotion data?</summary>
</br>

In order to make POST request, we'll be using the `node-fetch` package. [What is the node-fetch package?](https://www.npmjs.com/package/node-fetch) Install it!

<details>
<summary>‚ùì Why use Fetch?</summary>
</br>

We're calling the `fetch` function - notice the **await** keyword, which we need because `fetch` returns a **Promise**, which is a proxy for a value that isn't currently known. You can read about Javascript promises [here](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise). 

Javascript is what we call a "synchronous" language, meaning operations in Javascript block other operations from executing until they are complete, creating a sense of single directional flow. This means that only one operation can happen at a time. However, in order to maximize efficiency (save time and resources), Javascript allows the use of asynchronous functions.
<br>
</details>
<br>

<details>
<summary>‚ùì What is an async function?</summary>
</br>

Simply put, async functions allow other operations to continue running as they are being executed. Refer to [this site](https://dev.to/hardy613/asynchronous-vs-synchronous-programming-23ed) for more information.

Promises are sychnronous objects, similar to their real life meaning, return a value at some point in the future, or a reason for why that value could not be returned - they represent the result of an async function that may or may not be resolved.

> [Is JavaScript Synchronous or Asynchronous? What the Hell is a Promise?](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise)

> [Master the JavaScript Interview: What is a Promise?](https://medium.com/better-programming/is-javascript-synchronous-or-asynchronous-what-the-hell-is-a-promise-7aa9dd8f3bfb)
</br>
</details>
<br>

```js
//install the node-fetch pacakge
var fetch = '<CODE HERE>'
```
When you've finished installing, read through the [**API** section](https://www.npmjs.com/package/node-fetch#api) of the documentation. We're going to make a call using the `fetch(url, {options})` function.

> API Documentation can be tricky sometimes...Here's something to [help](https://learn.parabola.io/docs/reading-api-docs)
<br>

</details>
<br>

<details>
<summary>‚ùì What is the URL?</summary>
</br>

Notice that the URL is just the uriBase with the params we specified earlier appended on.

For now, fill in the `method`  and `body`.

```js
async function analyzeImage(img){
    
    const subscriptionKey = '<YOUR SUBSCRIPTION KEY>';
    const uriBase = '<YOUR ENDPOINT>' + '/face/v1.0/detect';

    let params = new URLSearchParams({
        'returnFaceId': 'true',
        'returnFaceAttributes': 'emotion'
    })

    //COMPLETE THE CODE
    let resp = await fetch(uriBase + '?' + params.toString(), {
        method: '<METHOD>',  //WHAT TYPE OF REQUEST?
        body: '<BODY>',  //WHAT ARE WE SENDING TO THE API?
        headers: {
            '<HEADER NAME>': '<HEADER VALUE>'  //do this in the next section
        }
    })

    let data = await resp.json();
    
    return data; 
}
```
</details>
<br>


<details>
<summary>‚ùì How do I specify Request Headers?</summary>
</br>

Go back to the Face API documentation [here](https://westus.dev.cognitive.microsoft.com/docs/services/563879b61984550e40cbbe8d/operations/563879b61984550f30395236), and find the **Request headers** section. 

There are two headers that you need. I've provided the format below. Enter in the two header names and their two corresponding values. 

FYI: The `Content-Type`  header should be set to`'application/octet-stream'`.  This specifies a binary file.

```js
    //COMPLETE THE CODE
    let resp = await fetch(uriBase + '?' + params.toString(), {
        method: '<METHOD>',  //WHAT TYPE OF REQUEST?
        body: '<BODY>',  //WHAT ARE WE SENDING TO THE API?
      
      	//ADD YOUR TWO HEADERS HERE
        headers: {
            '<HEADER NAME>': '<HEADER VALUE>'
        }
    })
```

</details>
<br>
<details>
<summary>‚ùìHow do I actually analyze the image?</summary>
</br>

Call the `analyzeImage` function in `module.exports`. Add the code below into `module.exports`.

Remember that `parts` represents the parsed multipart form data. It is an array of parts, each one described by a filename, a type and a data. Since we only sent one file, it is stored in index 0, and we want the `data`  property to access the binary file‚Äì hence `parts[0].data`. Then in the HTTP response of our Azure function, we store the result of the API call.

```js
//module.exports function

//analyze the image
var result = await analyzeImage(parts[0].data);

context.res = {
	body: {
		result
	}
};

console.log(result)
context.done(); 

```

</details>
<br>

<details>
<summary>‚ùóÔ∏è Test your function to see if it outputs face data!</summary>
<br>

* Navigate back to the Postman Chrome extension app and change GET to POST
![Untitled_ Nov 11, 2020 6_24 PM](https://user-images.githubusercontent.com/69332964/98876201-c3bca780-244b-11eb-9b94-8d3cecc80115.gif)

* Copy your function's url from the Azure Function App portal like this:
![httptrigger - Microsoft Azure](https://user-images.githubusercontent.com/69332964/98876502-6f65f780-244c-11eb-832b-a25888b980da.gif)

* Use the function url and any image you want to send the POST request. Remember to attach the file in Body!
![Untitled_ Nov 11, 2020 6_40 PM](https://user-images.githubusercontent.com/69332964/98876997-780afd80-244d-11eb-87fc-13822d909f2f.gif)

</details>
<br>

Make sure you're using an image with a *real face* on it or else it won't work. Here's an example of an output I get with this image:

![image](https://user-images.githubusercontent.com/69332964/98884689-91687580-245e-11eb-98d7-6461ac79e02a.jpg)
*Credits: https://thispersondoesnotexist.com/*

```json
{
  "result": [
    {
      "faceId": "d25465d6-0c38-4417-8466-cabdd908e756",
      "faceRectangle": {
        "top": 313,
        "left": 210,
        "width": 594,
        "height": 594
      },
      "faceAttributes": {
        "emotion": {
          "anger": 0,
          "contempt": 0,
          "disgust": 0,
          "fear": 0,
          "happiness": 1,
          "neutral": 0,
          "sadness": 0,
          "surprise": 0
        }
      }
    }
  ]
}
```