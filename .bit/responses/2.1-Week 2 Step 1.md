---
files: n/a
stepType: checks
scripts: test.parseimage.js
week: 2
step: 1
name: Week 2 Step 1
---
Week 2 Step 1 ‚¨§‚óØ‚óØ‚óØ‚óØ‚óØ‚óØ | üïê Estimated completion: 5-20 minutes

## Getting Emotional ~ With the Face API 
Because of amazing APIs, you don't need to be an expert in machine learning and AI to take advantage of cutting edge technology. In this project, we are going to be building an API to text you a GIF when you send in a picture of yourself!   

### ‚úÖ Task:
**Create an Azure function that can send a static image to the Face API that returns if a person has a beard or not.**
- [ ] Create a new branch named `week-2` (use this branch for all tasks in week 2)
- [ ] Create a Face [API Endpoint](https://smartbear.com/learn/performance-monitoring/api-endpoints/#:~:text=For%20APIs%2C%20an%20endpoint%20can,to%20carry%20out%20their%20function.&text=The%20place%20that%20APIs%20send,lives%2C%20is%20called%20an%20endpoint.)
- [ ] Build an Azure function that makes a request to the Face API and sends back the emotion data in a JSON function
- [ ] Place your function URL in a [repository secret](https://docs.github.com/en/actions/reference/encrypted-secrets#creating-encrypted-secrets-for-a-repository) called `EMOTIONAL_FUNCTION`
- [ ] Commit your code to `week2/emotional.js`

### üöß Test your Work
Use **Postman**! Paste the *function url* and make a POST request. Remember to attach the file in `Body`! In the output box, you should get the output. Make sure you're using an image with a *real face* on it or else it won't work. Here's an example of an output I get with [this image](https://user-images.githubusercontent.com/69332964/98884689-91687580-245e-11eb-98d7-6461ac79e02a.jpg):

<details>
<summary>
Expected Output 
</summary>

```json
{
  "result": [
    {
      "faceId": "d25465d6-0c38-4417-8466-cabdd908e756",
      "faceRectangle": {
        "top": 313,
        "left": 210,
        "width": 594,
        "height": 594
      },
      "faceAttributes": {
        "emotion": {
          "anger": 0,
          "contempt": 0,
          "disgust": 0,
          "fear": 0,
          "happiness": 1,
          "neutral": 0,
          "sadness": 0,
          "surprise": 0
        }
      }
    }
  ]
}
```

</details>

<details>
<summary>Confused about Postman? </summary>
* Navigate back to the Postman app and change GET to POST
![Untitled_ Nov 11, 2020 6_24 PM](https://user-images.githubusercontent.com/69332964/98876201-c3bca780-244b-11eb-9b94-8d3cecc80115.gif)

* Copy your function's url from the Azure Function App portal like this:
![httptrigger - Microsoft Azure](https://user-images.githubusercontent.com/69332964/98876502-6f65f780-244c-11eb-832b-a25888b980da.gif)

* Use the function url and any image you want to send the POST request. Remember to attach the file in Body!
![Untitled_ Nov 11, 2020 6_40 PM](https://user-images.githubusercontent.com/69332964/98876997-780afd80-244d-11eb-87fc-13822d909f2f.gif)
</details>

### The Face API 

The Face API will accept the image and return information about the face, specifically emotions. Watch this video on Microsoft Cognitive Services for an in-depth explanation: http://www.youtube.com/watch?v=2aA8OEZ1wk8 

<details>
<summary>‚ùì How do I create and access the Face API?</summary>
</br>

1. Log into your Azure portal
2. Navigate to **Create a Resource**, the **AI + Machine Learning** tab on the left, and finally select **Face** and fill out the necessary information
3. Record and save the API endpoint and [subscription key](https://docs.microsoft.com/en-us/azure/api-management/api-management-subscriptions)
4. Place the API endpoint and subscrition key in the GitHub repository secrets: `API_ENDPOINT` AND `SUBSCRIPTION_KEY`
    * These keys will be used in the Azure function to give access to this API

<br>

</details>
<details>
<summary>‚ùì Where can I find the Face API keys?</summary>
 </br>

1. Navigate to the home page on the Micrsoft Azure portal (https://portal.azure.com/#home)
<img width="1440" alt="Screen Shot 2021-02-04 at 4 00 33 PM" src="https://user-images.githubusercontent.com/28051494/106971033-edac0000-6702-11eb-8243-1b5c2318f76d.png">

2. Click on the resource you need the keys for
<img width="1438" alt="Screen Shot 2021-02-04 at 4 00 49 PM" src="https://user-images.githubusercontent.com/28051494/106971035-ef75c380-6702-11eb-965b-c3ef7b5a7574.png">

3. On the left menu bar, locate the Resource Management section and click on "Keys and Endpoint"
<img width="1440" alt="Screen Shot 2021-02-04 at 12 26 36 PM" src="https://user-images.githubusercontent.com/28051494/106971042-f43a7780-6702-11eb-9e28-e6b2bc16fa22.png">
<br>
</details>



### The Function

#### ‚è´ Requesting Data
<details>
<summary>‚ùì How do I make the request to the Face API?</summary>
</br>

Create a new function in you new Azure function index.js file, outside of  `module.exports`  that will handle analyzing the image (this function is **async** because we will be using the **await** keyword with the API call).

This function will be called `analyzeImage(img)` and takes in one parameter, `img`, that contains the image we're trying to analyze.  Inside, we have two variables involved in the call: `subscriptionKey`  and `uriBase`.  Substitute the necessary values with your own info.

```js
async function analyzeImage(img){
    const subscriptionKey = process.env.SUBSCRIPTIONKEY;
    const uriBase = process.env.ENDPOINT + '/face/v1.0/detect';
}
```
</details>
<br>

<details>
<summary>‚ùì What are the parameters for the request?</summary>
</br>

Checkout the [documentation for the Face API](https://westus.dev.cognitive.microsoft.com/docs/services/563879b61984550e40cbbe8d/operations/563879b61984550f30395236). Notice that the request url is this:

`https://{endpoint}/face/v1.0/detect\[?returnFaceId]\[&returnFaceLandmarks]\[&returnFaceAttributes]\[&recognitionModel]\[&returnRecognitionModel][&detectionModel]`

with the following parameters in [ ]:
* [?returnFaceId]
* [&returnFaceLandmarks]
* [&returnFaceAttributes]
* [&recognitionModel]
* [&returnRecognitionModel]
* [&detectionModel]

All of the bracketed sections represent possible request parameters. Read through **Request Parameters** section carefully. How can we specify that we want to get the emotion data?

</details>
<br>

<details>
<summary>‚ùóÔ∏è Specify parameters in your new function?</summary>
</br>

In order to specify all of our parameters easily, we're going to create a new `URLSearchParams`  object. Here's the object declared for you. I've also already specified one parameter, `returnFaceId`,  as `true` to provide an example. Add in a new parameter that requests emotion.

```js
let params = new URLSearchParams({
	'returnFaceId': 'true',
	'<PARAMETER NAME>': '<PARAMETER VALUE>'     //FILL IN THIS LINE
})
```

</details>
<br>

<details>
<summary>‚ùóÔ∏è Use Fetch in order to send a post request to the Face API Endpoint and receive emotion data?</summary>
</br>

In order to make POST request, we'll be using the `node-fetch` package. [What is the node-fetch package?](https://www.npmjs.com/package/node-fetch) Install it!

<details>
<summary>‚ùì Why use Fetch?</summary>
</br>

We're calling the `fetch` function - notice the **await** keyword, which we need because `fetch` returns a **Promise**, which is a proxy for a value that isn't currently known. You can read about Javascript promises [here](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise). 

Javascript is what we call a "synchronous" language, meaning operations in Javascript block other operations from executing until they are complete, creating a sense of single directional flow. This means that only one operation can happen at a time. However, in order to maximize efficiency (save time and resources), Javascript allows the use of asynchronous functions.
<br>
</details>
<br>

<details>
<summary>‚ùì What is an async function?</summary>
</br>

Simply put, async functions allow other operations to continue running as they are being executed. Refer to [this site](https://dev.to/hardy613/asynchronous-vs-synchronous-programming-23ed) for more information.

Promises are sychnronous objects, similar to their real life meaning, return a value at some point in the future, or a reason for why that value could not be returned - they represent the result of an async function that may or may not be resolved.

> [Is JavaScript Synchronous or Asynchronous? What the Heck is a Promise?](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise)

> [Master the JavaScript Interview: What is a Promise?](https://medium.com/better-programming/is-javascript-synchronous-or-asynchronous-what-the-hell-is-a-promise-7aa9dd8f3bfb)
</br>
</details>
<br>

```js
//install the node-fetch pacakge
var fetch = '<CODE HERE>'
```
When you've finished installing, read through the [**API** section](https://www.npmjs.com/package/node-fetch#api) of the documentation. We're going to make a call using the `fetch(url, {options})` function.

> API Documentation can be tricky sometimes...Here's something to [help](https://learn.parabola.io/docs/reading-api-docs)
<br>

</details>
<br>

<details>
<summary>‚ùì What is the URL?</summary>
</br>

Notice that the URL is just the uriBase with the params we specified earlier appended on.

For now, fill in the `method`  and `body`.

```js
async function analyzeImage(img){
    
    const subscriptionKey = '<YOUR SUBSCRIPTION KEY>';
    const uriBase = '<YOUR ENDPOINT>' + '/face/v1.0/detect';

    let params = new URLSearchParams({
        'returnFaceId': 'true',
        'returnFaceAttributes': 'emotion'
    })

    //COMPLETE THE CODE
    let resp = await fetch(uriBase + '?' + params.toString(), {
        method: '<METHOD>',  //WHAT TYPE OF REQUEST?
        body: '<BODY>',  //WHAT ARE WE SENDING TO THE API?
        headers: {
            '<HEADER NAME>': '<HEADER VALUE>'  //do this in the next section
        }
    })

    let data = await resp.json();
    
    return data; 
}
```
</details>
<br>


<details>
<summary>‚ùì How do I specify Request Headers?</summary>
</br>

Go back to the Face API documentation [here](https://westus.dev.cognitive.microsoft.com/docs/services/563879b61984550e40cbbe8d/operations/563879b61984550f30395236), and find the **Request headers** section. 

There are two headers that you need. I've provided the format below. Enter in the two header names and their two corresponding values. 

FYI: The `Content-Type`  header should be set to`'application/octet-stream'`.  This specifies a binary file.

```js
    //COMPLETE THE CODE
    let resp = await fetch(uriBase + '?' + params.toString(), {
        method: '<METHOD>',  //WHAT TYPE OF REQUEST?
        body: '<BODY>',  //WHAT ARE WE SENDING TO THE API?
      
      	//ADD YOUR TWO HEADERS HERE
        headers: {
            '<HEADER NAME>': '<HEADER VALUE>'
        }
    })
```

</details>
</br>

#### ‚è¨ Receiving Data
<details>
<summary>‚ùìHow do I actually analyze the image?</summary>
</br>

Call the `analyzeImage` function in `module.exports`. Add the code below into `module.exports`.

Remember that `parts` represents the parsed multipart form data. It is an array of parts, each one described by a filename, a type and a data. Since we only sent one file, it is stored in index 0, and we want the `data`  property to access the binary file‚Äì hence `parts[0].data`. Then in the HTTP response of our Azure function, we store the result of the API call.

```js
//module.exports function

//analyze the image
var result = await analyzeImage(parts[0].data);

context.res = {
	body: {
		result
	}
};

console.log(result)
context.done(); 

```

</details>
<br>

<details>
<summary>‚ùóÔ∏è Test your function to see if it outputs face data!</summary>
<br>

* Navigate back to the Postman Chrome extension app and change GET to POST
![Untitled_ Nov 11, 2020 6_24 PM](https://user-images.githubusercontent.com/69332964/98876201-c3bca780-244b-11eb-9b94-8d3cecc80115.gif)

* Copy your function's url from the Azure Function App portal like this:
![httptrigger - Microsoft Azure](https://user-images.githubusercontent.com/69332964/98876502-6f65f780-244c-11eb-832b-a25888b980da.gif)

* Use the function url and any image you want to send the POST request. Remember to attach the file in Body!
![Untitled_ Nov 11, 2020 6_40 PM](https://user-images.githubusercontent.com/69332964/98876997-780afd80-244d-11eb-87fc-13822d909f2f.gif)

</details>
<br>
